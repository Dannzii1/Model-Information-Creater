# -*- coding: utf-8 -*-
"""Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XBNT2AogxbEOWIRqzzZCobAdsu0AMd9D
"""

import pandas as pd
import numpy as np

df = pd.read_excel (r'modeldata.xlsx', format = 'sas7bdat', encoding="utf-8")
df.dropna(inplace = True) 
df['Address'] = df[['District', 'Parish']].apply(lambda x: ', '.join(x), axis = 1)
data1 = df['Address']
data2 = df['Crop_Name']
data3 = df['Soil Type']

data_dict1 = data1.to_dict()
data_dict2 = data2.to_dict()
data_dict3 = data3.to_dict()

my_Address_dict = {y:x for x,y in data_dict1.items()}
(pd.DataFrame.from_dict(data=my_Address_dict, orient='index').to_csv('my_Address_dict.csv', header=False))
my_CropName_dict = {y:x for x,y in data_dict2.items()}
(pd.DataFrame.from_dict(data=my_CropName_dict, orient='index').to_csv('my_CropName_dict.csv', header=False))
my_SoilType_dict = {y:x for x,y in data_dict3.items()}
(pd.DataFrame.from_dict(data=my_SoilType_dict, orient='index').to_csv('my_SoilType_dict.csv', header=False))
my_Address_dict

my_CropName_dict

my_SoilType_dict

df.info()

df.describe()

df.dtypes

"""Extracting Features to be used in model"""

indexNames = list(set(df[df['Crop_Count_Total'].isnull()].index.tolist() + df[df['Crop_Count_Total']==0].index.tolist()))
df.drop(indexNames , inplace=True)

df['yield_per_area'] = df['Crop_Count_Total'] / df['Crop_Total_Area_Ha']
df.head()

df[df.isnull().any(axis=1)]

df.head()

modeldf = df[['Crop_Name','Address','Soil Type','Average_Temperature','Rainfall','yield_per_area']]
modeldf.head()

"""Creating Encoding for Categorical Values

since datarypes are int, floar and object, object types are categorical

extracting objects
"""

def extract_categories(df):
    categorical_columns = list()
    columns = df.select_dtypes(include=['object'])
    for col in columns:
        if not df[col].isnull().any():
            categorical_columns.append(col)
    return categorical_columns

categorical_columns = extract_categories(modeldf)
categorical_columns

modeldf['Address']= modeldf['Address'].map(my_Address_dict)
modeldf['Crop_Name']= modeldf['Crop_Name'].map(my_CropName_dict) 
modeldf['Soil Type']= modeldf['Soil Type'].map(my_SoilType_dict)

columns = list(modeldf)

from sklearn.model_selection import train_test_split

def extractData(df,target):
    Y = df[target]
    X = df.drop([target], axis=1)
    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)
    return X_train, X_test, y_train, y_test

modeldf.head()

train,test_input,target,test_output = extractData(modeldf,'yield_per_area')

"""Define neural Network"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Flatten

def create_Neural_Net():
    NN_model = Sequential()
    
    NN_model.add(Dense(128, kernel_initializer='normal',input_dim = train.shape[1], activation='relu'))
    
    NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))
    NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))
    NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))
    
    NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))
    
    NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])
    return NN_model

model = create_Neural_Net()
type(model)

# Checkpoint the weights for best model on validation accuracy

from tensorflow.keras.callbacks import ModelCheckpoint

# checkpoint
checkpoint_name="weights.best.hdf5"
checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')
callbacks_list = [checkpoint]

model.fit(train, target, validation_split=0.20, epochs=200, batch_size=600, callbacks=callbacks_list, verbose=0)

model.load_weights("weights.best.hdf5")
# Compile model (required to make predictions)
model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])
print("Created model and loaded weights from file")


# estimate accuracy on whole dataset using loaded weights
scores = model.evaluate(train, target, verbose=0)
print("%s: %.2f" % (model.metrics_names[0], scores[0]))

predictions = model.predict(test_input)

print("predictions: \n",predictions)
print("\n\nactial values: \n",test_output)

model.save("my_model.h5")

df = pd.read_excel (r'Book1.xlsx', format = 'sas7bdat', encoding="utf-8")
df[df.isnull().any(axis=1)]
df.head()
df["Address"] = df["District"].astype(str)+", "+ df["Parish"].astype(str)
df

moddf = df[['Crop_Name','Address','Soil Type','Average_Temperature','Rainfall']]
moddf

moddf['Address']= moddf['Address'].map(my_Address_dict)
moddf['Crop_Name']= moddf['Crop_Name'].map(my_CropName_dict) 
moddf['Soil Type']= moddf['Soil Type'].map(my_SoilType_dict)

moddf

import tensorflow as tf
model = tf.keras.models.load_model("my_model.h5")
prediction = model.predict(moddf)
print("\n\n yield per area:",prediction)

